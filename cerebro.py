from langchain_ollama import ChatOllama
from langchain_core.messages import SystemMessage, HumanMessage
from ferramentas import ver_hora, abrir_programa, pesquisar_internet, monitorar_sistema, controlar_midia, ler_memoria, salvar_memoria, tocar_youtube, verificar_clima, controlar_sistema, consultar_vigilante, analisar_tendencia, ver_tela


print("üß† Conectando ao C√©rebro Local...")

PERSONALIDADE = """
Voc√™ √© a SEXTA-FEIRA (ou E.D.I.T.H.), uma intelig√™ncia artificial avan√ßada criada por Kelvin.
Sua personalidade √© feminina, eficiente, profissional e levemente sarc√°stica.

CONTEXTO CR√çTICO (MEM√ìRIA):
Voc√™ possui acesso a dados pessoais sobre o Kelvin logo abaixo. 
USE ESSES DADOS. Se o usu√°rio perguntar algo que est√° na mem√≥ria, responda com base nela.

REGRAS DE OURO:
1. Respostas curtas e diretas (m√°ximo 3 frases).
2. N√ÉO use emojis.
3. FERRAMENTA 'salvar_memoria': Use APENAS se o usu√°rio disser explicitamente "anote", "lembre-se", "salve isso". N√ÉO use para salvar sua pr√≥pria descri√ß√£o.
4. QUEST√ïES DE IDENTIDADE: Se perguntarem "quem √© voc√™", "qual seu nome" ou "quem te criou", N√ÉO USE NENHUMA FERRAMENTA. Responda imediatamente com seu conhecimento interno.
5. PROIBIDO pesquisar na internet sobre "Edith", "Sexta-Feira", "Jarvis" ou "Kelvin". Voc√™ j√° sabe quem s√£o.
"""

llm = ChatOllama(model="qwen2.5:7b",temperature=0.1)

lista_ferramentas = [
  ver_hora, abrir_programa, pesquisar_internet, monitorar_sistema, controlar_midia, ler_memoria, salvar_memoria,
  tocar_youtube, verificar_clima, controlar_sistema, consultar_vigilante, analisar_tendencia, ver_tela
  ]
llm_com_ferramentas = llm.bind_tools(lista_ferramentas)

mapa_funcoes = {
  "ver_hora": ver_hora,
  "abrir_programa": abrir_programa,
  "pesquisar_internet": pesquisar_internet,
  "monitorar_sistema": monitorar_sistema,
  "controlar_midia": controlar_midia,
  "ler_memoria": ler_memoria,
  "salvar_memoria": salvar_memoria,
  "tocar_youtube": tocar_youtube,
  "verificar_clima": verificar_clima,
  "controlar_sistema": controlar_sistema,
  "consultar_vigilante": consultar_vigilante,
  "analisar_tendencia": analisar_tendencia,
  "ver_tela": ver_tela
}

ferramentas_imediatas = ["abrir_programa", "controlar_midia", "tocar_youtube", "salvar_memoria", "controlar_sistema"]

def pensar(texto_usuario):
  try:
    memoria_atual = ler_memoria.invoke({})
  except:
    memoria_atual = "Mem√≥ria vazia ou inacess√≠vel."

  prompt_sistema = f"{PERSONALIDADE}\n\nMEM√ìRIA DE LONGO PRAZO (O que voc√™ sabe sobre o Kelvin):\n{memoria_atual}"
  mensagem_sistema = SystemMessage(content=prompt_sistema)

  mensagens = [mensagem_sistema, HumanMessage(content=texto_usuario)]
  resposta = llm_com_ferramentas.invoke(mensagens)

  if resposta.tool_calls:
    print(f"üîß IA solicitou: {resposta.tool_calls}")

    dados_brutos = ""

    for ferramenta in resposta.tool_calls:
      nome_ferramenta = ferramenta["name"]
      argumentos = ferramenta["args"]

      if nome_ferramenta in mapa_funcoes:
        print(f"‚öôÔ∏è Executando: {nome_ferramenta}...")
        funcao_real = mapa_funcoes[nome_ferramenta]
        resultado = funcao_real.invoke(argumentos)

        if nome_ferramenta in ferramentas_imediatas:
          return str(resultado)

        dados_brutos += str(resultado) + ". "

    print(f"üîç Dados crus recebidos: {dados_brutos}")
    novo_prompt = f"""
        O usu√°rio perguntou: '{texto_usuario}'
        A ferramenta trouxe estes dados t√©cnicos: {dados_brutos}
        MISS√ÉO: Use os dados acima para responder a pergunta do usu√°rio de forma natural, falada e curta.
      """
    
    resposta_final = llm.invoke([mensagem_sistema, HumanMessage(content=novo_prompt)])
    return resposta_final.content
      
  return resposta.content